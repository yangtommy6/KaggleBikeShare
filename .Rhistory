library(tidyverse) #specifically the dplyr library1
library(vroom)
library(lubridate)
bike <- vroom("/Users/christian/Desktop/STAT348/KaggleBikeShare/train.csv")
myCleanData <- bike %>%
mutate(hour = hour(datetime)) # Hour
library(tidymodels)
library(vroom)
library(lubridate)
library(tidymodels)
library(rpart)
train <- vroom("/Users/christian/Desktop/STAT348/KaggleBikeShare/train.csv")
test <- vroom("/Users/christian/Desktop/STAT348/KaggleBikeShare/test.csv")
my_recipe <- recipe(~., data=train) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
tree_model <- decision_tree(tree_depth = tune(), cost_complexity = tune(), min_n = tune()) %>%
set_engine("rpart") %>%
set_mode("regression")
tree_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(tree_model)
tree_grid <- grid_regular(
tree_depth(),
cost_complexity(),
min_n(),
levels = 5
)
cv_folds <- vfold_cv(train, v = 5)
tree_tune <- tune_grid(
tree_workflow,
resamples = cv_folds,
grid = tree_grid
)
best_params <- tree_tune %>%
select_best("mean")
#########
library(tidymodels)
tree_model <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use
set_mode("regression")
tree_workflow <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(tree_model)
## Set up grid of tuning values
tree_grid <- grid_regular(
tree_depth(),
cost_complexity(),
min_n(),
levels = c(5, 10, 15) # We can adjust the levels based on needs
)
## Set up K-fold CV
cv_folds <- vfold_cv(bikeTrain, v = 5) # 5-fold cross-validation
##Tune the model
tuned_tree <- tune_grid(
tree_workflow,
resamples = cv_folds,
grid = tree_grid,
metrics = metric_set(rmse, rsq)
)
